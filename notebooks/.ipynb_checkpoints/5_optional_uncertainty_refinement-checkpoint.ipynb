{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, torch, os\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "import copy, numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "import numpy.ma as ma\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def label_prop(xy,y_pred,y_std,y,th=0.04,k=35,alpha=0.01,max_iter=100,frac_uncertain=0.05,keep=False,n_iter=10,output_stats=False,use_threshold=False,average_technique=\"macro\",**kwargs):\n",
    "    if keep:\n",
    "        y_final=y_pred.argmax(1)\n",
    "    else:\n",
    "        pred_entropies = stats.distributions.entropy(y_pred.T)\n",
    "        y_new=copy.deepcopy(y_pred.argmax(1))\n",
    "        N=len(y_new)\n",
    "        idx=np.arange(N)\n",
    "        N_remove=int(frac_uncertain*N)\n",
    "        if not use_threshold:\n",
    "            uncertain_index = np.argsort(y_std.mean(1))[::-1][:N_remove]\n",
    "            y_new[uncertain_index]=-1\n",
    "            uncertain_index = np.argsort(pred_entropies)[::-1][:N_remove]\n",
    "            y_new[uncertain_index]=-1\n",
    "        else:\n",
    "            print(y_std.mean(1))\n",
    "            y_new[y_std.mean(1)>th]=-1\n",
    "        if n_iter:\n",
    "            for i in range(n_iter):\n",
    "                lp=LabelSpreading(kernel=\"knn\",n_neighbors=k,alpha=alpha,max_iter=max_iter).fit(xy,y_new)\n",
    "                y_final=lp.transduction_\n",
    "                pred_entropies=stats.distributions.entropy(lp.label_distributions_.T)\n",
    "                pred_entropies_new=pred_entropies\n",
    "                uncertain_index = np.argsort(pred_entropies)[::-1][:N_remove]\n",
    "                y_new=y_final\n",
    "                y_new[uncertain_index]=-1\n",
    "        else:\n",
    "            lp=LabelSpreading(kernel=\"knn\",n_neighbors=k,alpha=alpha,max_iter=max_iter).fit(xy,y_new)\n",
    "            y_final=y_new\n",
    "            y_final=lp.predict(xy)\n",
    "            pred_entropies_new=stats.distributions.entropy(lp.label_distributions_.T)\n",
    "\n",
    "\n",
    "    print(f1_score(y,y_pred.argmax(1),average=average_technique),f1_score(y,y_final,average=average_technique))\n",
    "    return y_final\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "no_pretrain=False\n",
    "cv_splits='cv_splits/cv_splits.pkl'\n",
    "cv_splits=pickle.load(open(cv_splits,'rb'))\n",
    "\n",
    "for j in range(10):\n",
    "    prediction_file=os.path.join(\"predictions,\"predictions.pth\")\n",
    "    print(prediction_file)\n",
    "    val_idx=cv_splits[j]['val_idx']\n",
    "    test_idx=cv_splits[j]['test_idx']\n",
    "    graphs=torch.load(prediction_file)\n",
    "    val_graphs=graphs[:len(val_idx)]\n",
    "    test_graphs=graphs[len(val_idx):]\n",
    "    args_=dict(th=0.05,alpha=0.05,k=8,max_iter=100,n_iter=(0),frac_uncertain=0.1,keep=True,use_threshold=False,average_technique=\"weighted\")\n",
    "    for i in range(len(graphs)):\n",
    "        print(i)\n",
    "        args=copy.deepcopy(args_)\n",
    "        args.update(graphs[i])\n",
    "        graphs[i]['y_update']=label_prop(**args)\n",
    "    torch.save(graphs,prediction_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
